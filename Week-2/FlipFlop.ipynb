{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating the Flip Flop task from the following [paper](https://direct.mit.edu/neco/article/25/3/626/7854/Opening-the-Black-Box-Low-Dimensional-Dynamics-in). The goal is to recreate it first using Gradient Descent (GD) to train the RNN, then using the Exponentiated Gradient (EG) method, and analyse any difference of results.\n",
    "\n",
    "To directly test produce the results, go to fixed-point-finder/examples/torch and run python run_FlipFlop.py\n",
    "\n",
    "In this notebook the aim is to recreate everything from \"scratch\". The key steps will involve: \n",
    "- Training the RNN\n",
    "- Generating the data to train / test the RNN on\n",
    "- Make a way to calculate the fixed points of the trained RNN \n",
    "- Make a way to visualise the fixed points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RNN \n",
    "Before generating the data I need to know what i want the RNN to look like (num entries, num exits, etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained a randomly connected network (N = 1000) to perform the 3-bit flip-flop task using the FORCE learning algorithm (Sussillo & Abbott) (see section 6). We then performed the linearization analysis, using the trajectories of the system during operation as ICs. Specifically, we spanned all possible transitions between the memory states using the inputs to the network and then randomly selected 600 network states out of these trajectories to serve as ICs for the q optimization. The algorithm resulted in 26 distinct fixed points, on which we performed a linear stability analysis. Specifically, we computed the Jacobian matrix, equation 3.12, around each fixed point and performed an eigenvector decomposition on these\n",
    "matrices. The resulting stable fixed points and saddle points are shown in Figure 3 (left). To display the results of these analyses, the network state x(t) is plotted in the basis of the first three principal components of the network activations (the transient pulses reside in other dimensions; one is shown in the right panel of Figure 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "\n",
    "class flipflop(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # We have to use super for the class to work as an nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        # We save the inputs/parameters here\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # We can't use the nn.rnn to define our RNN because it'd make everything learnable, so we have to implement it by hand\n",
    "        # We initialise the parameters (only one is learnable)\n",
    "        self.J = torch.randn(hidden_size, hidden_size) * 1 / math.sqrt(hidden_size)\n",
    "        self.B = torch.randn(hidden_size, input_size) * 0.1\n",
    "        self.W_fb = torch.randn(hidden_size, output_size) * 0.1\n",
    "        self.W_out = nn.Parameter(torch.randn(hidden_size, output_size), requires_grad=False)\n",
    "\n",
    "        # Initialise parameters for FORCE paramter update \n",
    "        alpha = 0.1\n",
    "        # Inverse correlation matrix\n",
    "        self.P = torch.eye(hidden_size) / alpha  \n",
    "\n",
    "\n",
    "    def forward(self, input_tensor, output_tensor, dt = 1.0):\n",
    "        '''  \n",
    "        This is the RNN's main time loop\n",
    "        This simulates the RNN over time and performs training at every step.\n",
    "        '''\n",
    "        # The input_tensor is the input of size [T, input size] where T is the number of time steps\n",
    "        T = input_tensor.shape[0]\n",
    "        device = self.W_out.device\n",
    "\n",
    "        # x is the hidden state of the RNN at the current time — it evolves over time according to the system's dynamics. In continuous-time terms, it’s like the system’s position in \"neural space.\"\n",
    "        x = torch.zeros(self.hidden_size, device=device)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # Training loop (dt is set to one to discretize time)\n",
    "        for t in range(T):\n",
    "            r = torch.tanh(x)\n",
    "            z = self.W_out.T @ r\n",
    "            x = (1 - dt) * x + dt * (self.J @ r + self.B @ input_tensor[t] + self.W_fb @ z)\n",
    "            self.force_update(r, z, output_tensor[t], dt)\n",
    "            outputs.append(z.detach())\n",
    "\n",
    "        return torch.stack(outputs)\n",
    "\n",
    "    def force_update(self, r, z, target, dt = 1.0):\n",
    "        \"\"\"\n",
    "        r: firing rates at current time step [hidden_size]\n",
    "        z: current output [output_size]\n",
    "        target: desired output [output_size]\n",
    "        dt: time step (optional but included for consistency)\n",
    "        \"\"\"\n",
    "        # The output weights W_out get updated, using Recursive Least Squares (RLS)\n",
    "        # Ensure correct shapes\n",
    "        r = r.unsqueeze(1)              # [hidden_size, 1]\n",
    "        e = (z - target).unsqueeze(0)   # [1, output_size]\n",
    "\n",
    "        # Compute gain vector k\n",
    "        Pr = self.P @ r                 # [hidden_size, 1]\n",
    "        rPr = (r.T @ Pr).item()         # scalar\n",
    "        c = 1.0 / (1.0 + rPr)\n",
    "        k = c * Pr                      # [hidden_size, 1]\n",
    "\n",
    "        # Update output weights: W_out = W_out - k * e\n",
    "        delta_W = k @ e                # [hidden_size, output_size]\n",
    "        self.W_out.data -= delta_W     # update in-place (no autograd)\n",
    "\n",
    "        # Update P matrix\n",
    "        self.P = self.P - k @ (r.T @ self.P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the data\n",
    "I need to create a [T, input_size] tensor of random -1, 0, 1. Ideally have them constant then have a random chance to switch to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_generation():\n",
    "    def __init__(self, T, input_size, prob, seed = 42):\n",
    "        # We initialise the values: \n",
    "        self.T = T\n",
    "        self.input_size = input_size\n",
    "        self.prob = prob\n",
    "        self.seed = seed\n",
    "    \n",
    "    def generate_flipflop_input(T, input_size, prob=0.01, seed):\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Initialise the input for the RNN\n",
    "        input_tensor = torch.zeros(T, input_size, dtype=torch.float32)\n",
    "\n",
    "        # Initial value: randomly chosen for each input bit from {-1, 0, 1}\n",
    "        current = torch.randint(low=-1, high=2, size=(input_size,), dtype=torch.int)\n",
    "\n",
    "        for t in range(T):\n",
    "            if t > 0:\n",
    "                # Carry over previous values\n",
    "                current = input_tensor[t - 1].clone().to(torch.int)\n",
    "\n",
    "            # Decide which bits to change\n",
    "            change_mask = torch.rand(input_size) < prob\n",
    "\n",
    "            for i in range(input_size):\n",
    "                if change_mask[i]:\n",
    "                    # Pick a new value that's different from the current one\n",
    "                    possible = [-1, 0, 1]\n",
    "                    possible.remove(int(current[i].item()))\n",
    "                    new_value = possible[torch.randint(len(possible), (1,)).item()]\n",
    "                    current[i] = new_value\n",
    "\n",
    "            input_tensor[t] = current\n",
    "\n",
    "        return input_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# We set the hidden size to 1000 as specified in the paper, and in the input/ouput sizes to 3 for the 3-bit flip flop task. \n",
    "hidden_size = 1000 \n",
    "input_size = 3\n",
    "output_size = 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
